{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a398a1c3",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8712332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import copy\n",
    "import dateutil\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "@dataclass\n",
    "class FileSize:\n",
    "    num_bytes: int = -1\n",
    "    nodes: int = -1\n",
    "\n",
    "@dataclass\n",
    "class ReducedFile:\n",
    "    num_bytes: int = -1\n",
    "    predicate_calls: int = -1\n",
    "    predicate_fails: int = -1\n",
    "    time_seconds: int = -1\n",
    "\n",
    "# Load the benchmark file sizes before they are reduced\n",
    "df = pd.read_csv(\"file_sizes.csv\")\n",
    "file_sizes = {}\n",
    "for row in df.iterrows():\n",
    "    err_nodes = row[1][3]\n",
    "    assert err_nodes == 0, \"Tree-Sitter could not parse one of the benchmark files\"\n",
    "    file_sizes[row[1][0]] = FileSize(row[1][1], row[1][2])\n",
    "\n",
    "for key in file_sizes.keys(): print(key, file_sizes[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6324ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv files generated by run_benchmarks.py\n",
    "# In each generated csv file has 4 columns:\n",
    "# time_before, time_after, size_bytes, return_code\n",
    "REDUCERS = [\"creduce\", \"perses\", \"bric-ddmin\", \"bric-hdd\", \"bric-br\", \"bric-gbr\"]\n",
    "reducers_results = {}\n",
    "for benchmark_name in glob.glob(\"**/*_output.csv\"):\n",
    "    benchmark = pd.read_csv(benchmark_name, header=None)\n",
    "    reduced_file = ReducedFile()\n",
    "    reduced_file.predicate_calls = len(benchmark.index)\n",
    "    reduced_file.predicate_fails = len(benchmark.loc[benchmark[3] != 0].values)\n",
    "\n",
    "    time_start = dateutil.parser.parse(benchmark[0][0])\n",
    "    time_end = dateutil.parser.parse(benchmark.iloc[-1][1])\n",
    "    reduced_file.time_seconds = (time_end - time_start).total_seconds()\n",
    "\n",
    "    last_successful_predicate = None\n",
    "    for row in reversed(list(benchmark.iterrows())):\n",
    "        predicate_return_code = row[1][3]\n",
    "        if predicate_return_code == 0:\n",
    "            last_successful_predicate = row[1]\n",
    "            break\n",
    "\n",
    "    if last_successful_predicate is None:\n",
    "        print(f\"{benchmark_name} None\")\n",
    "    else:\n",
    "        reduced_file.num_bytes = last_successful_predicate[2]\n",
    "        print(\n",
    "            f\"{benchmark_name:<40} {reduced_file.num_bytes:<10} {reduced_file.time_seconds}s {reduced_file.predicate_calls}/{reduced_file.predicate_fails}\"\n",
    "        )\n",
    "        reducers_results[benchmark_name] = copy.deepcopy(reduced_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
